{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-07T13:17:41.436231Z",
     "iopub.status.busy": "2022-07-07T13:17:41.435247Z",
     "iopub.status.idle": "2022-07-07T13:19:21.230772Z",
     "shell.execute_reply": "2022-07-07T13:19:21.229880Z",
     "shell.execute_reply.started": "2022-07-07T13:17:41.436151Z"
    },
    "id": "WQlSJOuHYIax",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib==3.3.4\n",
      "  Downloading matplotlib-3.3.4-cp38-cp38-manylinux1_x86_64.whl (11.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.6/11.6 MB\u001B[0m \u001B[31m20.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting numpy==1.19.5\n",
      "  Downloading numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.9/14.9 MB\u001B[0m \u001B[31m19.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting pandas==1.4.0\n",
      "  Downloading pandas-1.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.7/11.7 MB\u001B[0m \u001B[31m20.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting seaborn==0.11.2\n",
      "  Downloading seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m292.8/292.8 KB\u001B[0m \u001B[31m21.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting tensorflow-gpu==2.7.0\n",
      "  Downloading tensorflow_gpu-2.7.0-cp38-cp38-manylinux2010_x86_64.whl (489.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m489.6/489.6 MB\u001B[0m \u001B[31m3.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting cleverhans==4.0.0\n",
      "  Downloading cleverhans-4.0.0-py3-none-any.whl (92 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m92.3/92.3 KB\u001B[0m \u001B[31m20.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting pymoo==0.4.0\n",
      "  Downloading pymoo-0.4.0.tar.gz (511 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m511.7/511.7 KB\u001B[0m \u001B[31m21.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hCollecting scikit-learn==1.0.2\n",
      "  Downloading scikit_learn-1.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m26.7/26.7 MB\u001B[0m \u001B[31m17.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: tqdm==4.62.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 9)) (4.62.3)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 10)) (0.3.4)\n",
      "Collecting uncertainty_wizard\n",
      "  Downloading uncertainty_wizard-0.3.0-py3-none-any.whl (48 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.0/49.0 KB\u001B[0m \u001B[31m5.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting plot_keras_history\n",
      "  Downloading plot_keras_history-1.1.36.tar.gz (9.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.3.4->-r requirements.txt (line 1)) (3.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.3.4->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.3.4->-r requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.3.4->-r requirements.txt (line 1)) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.3.4->-r requirements.txt (line 1)) (9.0.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas==1.4.0->-r requirements.txt (line 3)) (2021.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.8/dist-packages (from seaborn==0.11.2->-r requirements.txt (line 4)) (1.4.1)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (0.4.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.21.0\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.4/2.4 MB\u001B[0m \u001B[31m17.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting libclang>=9.0.1\n",
      "  Downloading libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.5/14.5 MB\u001B[0m \u001B[31m20.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (1.1.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (0.35.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (3.17.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (1.15.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (1.1.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (1.12.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (2.7.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (3.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (0.2.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (1.12)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (0.12.0)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (2.7.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (3.3.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (2.7.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (1.39.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (3.7.4.3)\n",
      "Collecting easydict\n",
      "  Downloading easydict-1.9.tar.gz (6.4 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hCollecting pycodestyle\n",
      "  Downloading pycodestyle-2.8.0-py2.py3-none-any.whl (42 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m42.1/42.1 KB\u001B[0m \u001B[31m10.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from cleverhans==4.0.0->-r requirements.txt (line 6)) (1.1.0)\n",
      "Collecting nose\n",
      "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m154.7/154.7 KB\u001B[0m \u001B[31m27.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting tensorflow-probability\n",
      "  Downloading tensorflow_probability-0.17.0-py2.py3-none-any.whl (6.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.5/6.5 MB\u001B[0m \u001B[31m20.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting mnist\n",
      "  Downloading mnist-0.2.2-py2.py3-none-any.whl (3.5 kB)\n",
      "Collecting autograd>=1.3\n",
      "  Downloading autograd-1.4-py3-none-any.whl (48 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m48.8/48.8 KB\u001B[0m \u001B[31m9.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting cma==2.7\n",
      "  Downloading cma-2.7.0-py2.py3-none-any.whl (239 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m239.1/239.1 KB\u001B[0m \u001B[31m21.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==1.0.2->-r requirements.txt (line 8)) (3.1.0)\n",
      "Collecting sanitize_ml_labels>=1.0.33\n",
      "  Downloading sanitize_ml_labels-1.0.43.tar.gz (321 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m321.1/321.1 KB\u001B[0m \u001B[31m20.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hRequirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.8/dist-packages (from autograd>=1.3->pymoo==0.4.0->-r requirements.txt (line 7)) (0.18.2)\n",
      "Collecting compress_json\n",
      "  Downloading compress_json-1.0.7.tar.gz (5.4 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (2.6.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (3.3.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (2.0.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (59.4.0)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability->cleverhans==4.0.0->-r requirements.txt (line 6)) (2.0.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability->cleverhans==4.0.0->-r requirements.txt (line 6)) (5.1.1)\n",
      "Collecting dm-tree\n",
      "  Downloading dm_tree-0.1.7-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (142 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m142.6/142.6 KB\u001B[0m \u001B[31m22.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (5.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (4.10.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (2.0.11)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (3.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow-gpu==2.7.0->-r requirements.txt (line 5)) (3.2.0)\n",
      "Building wheels for collected packages: pymoo, plot_keras_history, sanitize_ml_labels, easydict, compress_json\n",
      "  Building wheel for pymoo (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for pymoo: filename=pymoo-0.4.0-cp38-cp38-linux_x86_64.whl size=1741418 sha256=001c977e15c4e2a007ba67d9131179d98915a85e77d15fb1cae8948d681ffc8b\n",
      "  Stored in directory: /root/.cache/pip/wheels/7c/5d/d1/b4d13eecdba15202701cca03cd5a4a70fe9a6fc56676e5d2a6\n",
      "  Building wheel for plot_keras_history (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for plot_keras_history: filename=plot_keras_history-1.1.36-py3-none-any.whl size=9238 sha256=2a58112a63530b666d398ffa7368a8f292cd2277453087a92e99b1a6bf1f24cd\n",
      "  Stored in directory: /root/.cache/pip/wheels/23/57/9c/9361218151ba1b3372b10fdcc5be07966041bc29b30476b686\n",
      "  Building wheel for sanitize_ml_labels (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for sanitize_ml_labels: filename=sanitize_ml_labels-1.0.43-py3-none-any.whl size=319231 sha256=8aecbe21ce573a71a158720b0cce5257e5b6f22c389a07b527706e70eada028f\n",
      "  Stored in directory: /root/.cache/pip/wheels/2c/f4/1e/e9284df82478d244e6a01dc246bf6d50f15384777d2a80eb51\n",
      "  Building wheel for easydict (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for easydict: filename=easydict-1.9-py3-none-any.whl size=6361 sha256=f331fd14b0cb2b0a676b29e928fc71f76bc22b6b73b9bc9035be1322f5131b3e\n",
      "  Stored in directory: /root/.cache/pip/wheels/d3/e0/e9/305e348717e399665119bd012510d51ff4f22d709ff60c3096\n",
      "  Building wheel for compress_json (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for compress_json: filename=compress_json-1.0.7-py3-none-any.whl size=5234 sha256=db8eeb64acdd98708b9861fc869dd64a5bd0255b0132bc18a9f78f8ca1721e6e\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/a2/2c/c9def2f8bc67f3bba74bf51218b5135be4daab11e34f947749\n",
      "Successfully built pymoo plot_keras_history sanitize_ml_labels easydict compress_json\n",
      "Installing collected packages: nose, libclang, easydict, dm-tree, compress_json, cma, uncertainty_wizard, tensorflow-io-gcs-filesystem, sanitize_ml_labels, pycodestyle, numpy, tensorflow-probability, pandas, mnist, matplotlib, autograd, seaborn, scikit-learn, pymoo, plot_keras_history, cleverhans, tensorflow-gpu\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.4\n",
      "    Uninstalling numpy-1.19.4:\n",
      "      Successfully uninstalled numpy-1.19.4\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.2.5\n",
      "    Uninstalling pandas-1.2.5:\n",
      "      Successfully uninstalled pandas-1.2.5\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.5.0\n",
      "    Uninstalling matplotlib-3.5.0:\n",
      "      Successfully uninstalled matplotlib-3.5.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.0\n",
      "    Uninstalling scikit-learn-0.24.0:\n",
      "      Successfully uninstalled scikit-learn-0.24.0\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dask-cudf 21.12.0a0+293.g0930f712e6 requires pandas<1.4.0dev0,>=1.0, but you have pandas 1.4.0 which is incompatible.\n",
      "cudf 21.12.0a0+293.g0930f712e6 requires pandas<1.4.0dev0,>=1.0, but you have pandas 1.4.0 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed autograd-1.4 cleverhans-4.0.0 cma-2.7.0 compress_json-1.0.7 dm-tree-0.1.7 easydict-1.9 libclang-14.0.1 matplotlib-3.3.4 mnist-0.2.2 nose-1.3.7 numpy-1.19.5 pandas-1.4.0 plot_keras_history-1.1.36 pycodestyle-2.8.0 pymoo-0.4.0 sanitize_ml_labels-1.0.43 scikit-learn-1.0.2 seaborn-0.11.2 tensorflow-gpu-2.7.0 tensorflow-io-gcs-filesystem-0.26.0 tensorflow-probability-0.17.0 uncertainty_wizard-0.3.0\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: You are using pip version 22.0.3; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-07T13:29:55.732592Z",
     "iopub.status.busy": "2022-07-07T13:29:55.731640Z",
     "iopub.status.idle": "2022-07-07T13:30:06.220088Z",
     "shell.execute_reply": "2022-07-07T13:30:06.219153Z",
     "shell.execute_reply.started": "2022-07-07T13:29:55.732537Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-4.5.1.tar.gz (14 kB)\n",
      "  Installing build dependencies ... \u001B[?25ldone\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\n",
      "\u001B[?25hRequirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.4.2)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m128.2/128.2 KB\u001B[0m \u001B[31m14.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.62.3)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.27.1)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.0.11)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2021.10.8)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Building wheels for collected packages: gdown\n",
      "  Building wheel for gdown (pyproject.toml) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for gdown: filename=gdown-4.5.1-py3-none-any.whl size=14951 sha256=a2a0833f9d9a6530bcdebff51dd0795d941b2c46e548c3d465a442a34f4ec945\n",
      "  Stored in directory: /root/.cache/pip/wheels/8d/df/71/846b2aa0fabaac2af23fbc5214eeaa55f0616e9d1a05187d72\n",
      "Successfully built gdown\n",
      "Installing collected packages: soupsieve, PySocks, beautifulsoup4, gdown\n",
      "Successfully installed PySocks-1.7.1 beautifulsoup4-4.11.1 gdown-4.5.1 soupsieve-2.3.2.post1\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: You are using pip version 22.0.3; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import uncertainty_wizard as uwiz\n",
    "from cleverhans.tf2.attacks.fast_gradient_method import fast_gradient_method\n",
    "from pymoo.algorithms.nsga2 import NSGA2\n",
    "from pymoo.factory import get_sampling, get_crossover, get_mutation\n",
    "from pymoo.model.problem import Problem\n",
    "from pymoo.optimize import minimize\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from plot_keras_history import plot_history\n",
    "import warnings\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-07T13:33:20.433759Z",
     "iopub.status.busy": "2022-07-07T13:33:20.433435Z",
     "iopub.status.idle": "2022-07-07T13:35:02.922337Z",
     "shell.execute_reply": "2022-07-07T13:35:02.921377Z",
     "shell.execute_reply.started": "2022-07-07T13:33:20.433730Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ZYlhSfZ_fAZLE0RNyxgyENqQNSfZVDHu\n",
      "To: /notebooks/X\n",
      "100%|██████████████████████████████████████| 1.96G/1.96G [01:40<00:00, 19.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown --id 1ZYlhSfZ_fAZLE0RNyxgyENqQNSfZVDHu\n",
    "!gdown --id 1Nzx8gOSlGnVdV1QY1YBvOKCG0UQsn_kp\n",
    "with open('data.txt') as f:\n",
    "    data=f.readlines()\n",
    "data[1]=data[1].split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_SIZE=int(data[1][0])\n",
    "SAMPLE_SIZE =int(data[1][1])\n",
    "percentage=float(data[1][2])\n",
    "noise_budget =float(data[1][3])\n",
    "T=int(data[1][4])\n",
    "alpha=float(data[1][5])\n",
    "beta=float(data[1][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 40 1.0 0.3 40 0.9 0.3\n"
     ]
    }
   ],
   "source": [
    "print(EXPERIMENT_SIZE,SAMPLE_SIZE,percentage,noise_budget,T,alpha,beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T23:26:31.497770Z",
     "iopub.status.busy": "2022-07-04T23:26:31.497353Z",
     "iopub.status.idle": "2022-07-04T23:26:31.503952Z",
     "shell.execute_reply": "2022-07-04T23:26:31.502606Z",
     "shell.execute_reply.started": "2022-07-04T23:26:31.497770Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9 0.3 40 40\n"
     ]
    }
   ],
   "source": [
    "print(alpha,beta,EXPERIMENT_SIZE,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T23:26:32.378179Z",
     "iopub.status.busy": "2022-07-04T23:26:32.377832Z",
     "iopub.status.idle": "2022-07-04T23:26:32.391243Z",
     "shell.execute_reply": "2022-07-04T23:26:32.389744Z",
     "shell.execute_reply.started": "2022-07-04T23:26:32.378143Z"
    },
    "id": "NaQQiEC2QkGl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class UncertaintyMaximization(Problem):\n",
    "    def __init__(self,input_instance,y, epsilon,beta,alpha):\n",
    "        super().__init__(n_var=X_train.shape[1],\n",
    "                         n_obj=1,\n",
    "                         n_constr=0,\n",
    "                         elementwise_evaluation=True)\n",
    "        self.input_instance = input_instance\n",
    "        self.y = y\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha=alpha\n",
    "        self.beta=beta\n",
    "    def _evaluate(self, noise, out, *args, **kwargs):\n",
    "        prev_results = model.predict_quantified(self.input_instance,\n",
    "                                       quantifier=quantifiers,\n",
    "                                       batch_size=50000,\n",
    "                                       sample_size=T,\n",
    "                                       verbose=0)\n",
    "        \n",
    "        tmp_instance = self.input_instance + noise\n",
    "        results = model.predict_quantified(tmp_instance,\n",
    "                                       quantifier=quantifiers,\n",
    "                                       batch_size=50000,\n",
    "                                       sample_size=T,\n",
    "                                       verbose=0)\n",
    "        unc = results[0][1][0]\n",
    "        y_hat = results[0][0][0]\n",
    "        f1 = self.alpha/(0.0000001 + unc) + 0.01 * np.linalg.norm(noise) + self.beta * (np.int(self.y)==np.int(y_hat))\n",
    "        g1 = np.linalg.norm(noise, ord=np.inf) - self.epsilon\n",
    "        out[\"F\"] = np.column_stack([f1])\n",
    "        out[\"G\"] = np.column_stack([g1])\n",
    "        \n",
    "        #print('U1:', prev_results[0][1][0],'U2:', unc,'Obj:',f1)\n",
    "def get_bounds(uncertainty_budget):\n",
    "    distance = np.max(X_train) - np.min(X_train)\n",
    "    b = (-1*distance*uncertainty_budget/2.0, distance*uncertainty_budget/2.0)\n",
    "\n",
    "    bounds = [b for i in range(X_train.shape[1])]\n",
    "\n",
    "    initial_noise = np.random.uniform(low=np.min(bounds),\n",
    "                                      high=np.max(bounds),\n",
    "                                      size=(X_train.shape[1],))\n",
    "\n",
    "    return bounds,initial_noise\n",
    "def get_model():\n",
    "    model = uwiz.models.StochasticSequential()\n",
    "    model.add(tf.keras.layers.Dense(100, activation='relu',input_dim=X_train.shape[1]))\n",
    "    model.add(tf.keras.layers.Dropout(0.09))\n",
    "    model.add(tf.keras.layers.Dense(200, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.09))\n",
    "    model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'],run_eagerly=True)\n",
    "    return model\n",
    "def generate_instances(beta,alpha,noise_budget = 0.05, sample_size = 10):#beta=0.1,alpha=0.1):\n",
    "    bounds,initial_noise = get_bounds(noise_budget)\n",
    "    prev_unc_list = []\n",
    "    unc_list = []\n",
    "    prev_pred_list = []\n",
    "    pred_list = []\n",
    "    noise_norm = []\n",
    "    org_x = []\n",
    "    perturbated_x = []\n",
    "    original_y = []\n",
    "    input_norm = []\n",
    "    selected_idx = []\n",
    "    y_tmp_onehot_list = []\n",
    "\n",
    "    for j in tqdm(range(sample_size)):\n",
    "        #i = np.random.randint(0,X_train.shape[0],1)[0]\n",
    "        i=j\n",
    "        #print('i',i)\n",
    "        selected_idx.append(i)\n",
    "        x_tmp = np.array([X_train[i,:],])\n",
    "        y_tmp = y_train[i].argmax()\n",
    "        y_tmp_onehot = y_train[i]\n",
    "\n",
    "        prev_results = model.predict_quantified(x_tmp,\n",
    "                                               quantifier=quantifiers,\n",
    "                                               batch_size=50000,\n",
    "                                               sample_size=30,\n",
    "                                               verbose=0)\n",
    "        problem = UncertaintyMaximization(input_instance=x_tmp,\n",
    "                                          y=y_tmp,\n",
    "                                          epsilon=noise_budget,alpha=alpha,beta=beta)\n",
    "        problem.xl = np.ones((x_tmp.shape[1],)) * np.min(bounds)\n",
    "        problem.xu = np.ones((x_tmp.shape[1],)) * np.max(bounds)\n",
    "\n",
    "        algorithm = NSGA2(\n",
    "                            pop_size=40,\n",
    "                            n_offsprings=10,\n",
    "                            sampling=get_sampling(\"real_random\"),\n",
    "                            crossover=get_crossover(\"real_sbx\", prob=0.9, eta=15),\n",
    "                            mutation=get_mutation(\"real_pm\", eta=20),\n",
    "                            eliminate_duplicates=True\n",
    "                        )\n",
    "\n",
    "        res = minimize(problem,\n",
    "                       algorithm,\n",
    "                       (\"n_gen\", 10),\n",
    "                       verbose=False)\n",
    "\n",
    "        ##############################\n",
    "        ###     Noise generated    ###\n",
    "        ##############################\n",
    "\n",
    "        res1 = model.predict_quantified(x_tmp,\n",
    "                                               quantifier=quantifiers,\n",
    "                                               batch_size=50000,\n",
    "                                               sample_size=T,\n",
    "                                               verbose=0)\n",
    "        x_tmp_noisy = x_tmp + res.X\n",
    "\n",
    "        results = model.predict_quantified(x_tmp_noisy,\n",
    "                                           quantifier=quantifiers,\n",
    "                                           batch_size=50000,\n",
    "                                           sample_size=T,\n",
    "                                           verbose=0)\n",
    "        #print('U1:', res1[0][1][0],'U2:', results[0][1][0])\n",
    "        prev_unc_list.append(prev_results[0][1][0])\n",
    "        prev_pred_list.append(prev_results[0][0][0].astype(int))\n",
    "\n",
    "        unc_list.append(results[0][1][0])\n",
    "        pred_list.append(results[0][0][0].astype(int))\n",
    "\n",
    "        noise_norm.append(np.linalg.norm(res.X))\n",
    "        input_norm.append(np.linalg.norm(x_tmp))\n",
    "        original_y.append(y_tmp)\n",
    "        y_tmp_onehot_list.append(y_tmp_onehot)\n",
    "\n",
    "        org_x.append(x_tmp)\n",
    "        perturbated_x.append(x_tmp_noisy)\n",
    "\n",
    "    df = pd.DataFrame({'idx':selected_idx,'y':original_y,\n",
    "                       'y_hat':prev_pred_list,'y_hat2':pred_list,\n",
    "                       'unc_init':prev_unc_list,'unc_pert':unc_list,\n",
    "                       'input_norm':input_norm,'noise_norm':noise_norm\n",
    "                    })\n",
    "    np.savez(f'train_test_{alpha}_{beta}-' + str(noise_budget) + '.npz',\n",
    "                     X_train=X_train, X_test=X_test, y_train=y_train,\n",
    "                     y_test=y_test, org_x=org_x, perturbated_x=perturbated_x,\n",
    "                     original_y=original_y,prev_pred_list=prev_pred_list,\n",
    "                     pred_list=pred_list,y_tmp_onehot_list=y_tmp_onehot_list)\n",
    "    df.to_csv(f'manipulated_instances_{alpha}_{beta}.csv',index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T23:26:34.245731Z",
     "iopub.status.busy": "2022-07-04T23:26:34.245397Z",
     "iopub.status.idle": "2022-07-04T23:26:35.769046Z",
     "shell.execute_reply": "2022-07-04T23:26:35.768122Z",
     "shell.execute_reply.started": "2022-07-04T23:26:34.245703Z"
    },
    "id": "A53rJo6wTEr6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245818\n"
     ]
    }
   ],
   "source": [
    "# demonstrate data normalization with sklearn\n",
    "# load data\n",
    "data = ...\n",
    "# create scaler\n",
    "scaler = MinMaxScaler()\n",
    "infileX = open(\"X\",'rb')\n",
    "X = pickle.load(infileX)\n",
    "infiley = open(\"y\",'rb')\n",
    "y = pickle.load(infiley)\n",
    "length=int(len(X)*percentage)\n",
    "Xs=X[:length]\n",
    "Ys=y[:length]\n",
    "print(len(Ys))\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, Ys,\n",
    "                                                    test_size=0.33, random_state=27)\n",
    "quantifiers = ['pred_entropy']\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T23:26:43.719291Z",
     "iopub.status.busy": "2022-07-04T23:26:43.719001Z"
    },
    "id": "3a161rZyhfKE",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      9037\n",
      "           1       0.00      0.00      0.00      9098\n",
      "           2       0.29      0.00      0.01      8853\n",
      "           3       0.00      0.00      0.00      9062\n",
      "           4       0.00      0.00      0.00      9020\n",
      "           5       0.00      0.00      0.00      9141\n",
      "           6       0.10      0.84      0.18      8909\n",
      "           7       0.00      0.00      0.00      9052\n",
      "           8       0.17      0.12      0.14      8948\n",
      "\n",
      "    accuracy                           0.11     81120\n",
      "   macro avg       0.06      0.11      0.04     81120\n",
      "weighted avg       0.06      0.11      0.04     81120\n",
      "\n",
      "**************************************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc27b3713318451b9f87729d7e464654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698352d2d9c94e988a096a6fe68beb90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314ab7425fe34256bf70f0a665e41316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63d4ca6732749ab889a12d745103282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50a6c848ae941dd92c6b27a2c9aa53e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39effe0626b41718909b05ac62966cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f052b4e5150b444ab0092dabcf912450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00005: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8298838a1c244c8e8d367ba0d386af5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3eef66f2ddb46a39357ea5665606dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00008: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679bc2e2233c43a4a5fee87402c840ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d8d38ccaa34e2b84a78aa5463918c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Generate instances\n",
    "tf.config.run_functions_eagerly(True)\n",
    "y_hat = model.predict(X_test)\n",
    "cr = classification_report(y_test.argmax(axis=1),y_hat.argmax(axis=1))\n",
    "print(cr)\n",
    "print('*'*50)\n",
    "prec_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "acc_list = []\n",
    "\n",
    "prec_list.append(precision_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "recall_list.append(recall_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "f1_list.append(f1_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "acc_list.append(accuracy_score(y_test.argmax(axis=1),y_hat.argmax(axis=1)))\n",
    "\n",
    "\n",
    "es_unc = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "\n",
    "model.inner.load_weights('model.hdf5')\n",
    "\n",
    "for _ in range(EXPERIMENT_SIZE):\n",
    "    df = generate_instances(noise_budget = noise_budget, sample_size=T,beta=beta,alpha=alpha)\n",
    "    idx = df.idx.values\n",
    "    if len(idx) > 0:\n",
    "        load_data = np.load(f'train_test_{alpha}_{beta}-' + str(noise_budget) + '.npz')\n",
    "        perturbated_x = load_data['perturbated_x']\n",
    "        original_y = load_data['original_y']\n",
    "        X_train = load_data['X_train']\n",
    "        y_train = load_data['y_train']\n",
    "        X_test = load_data['X_test']\n",
    "        y_test = load_data['y_test']\n",
    "        y_tmp_onehot_list = load_data['y_tmp_onehot_list']\n",
    "\n",
    "        perturbated_x = perturbated_x.reshape((perturbated_x.shape[0],perturbated_x.shape[2]))\n",
    "\n",
    "        new_X_train = np.concatenate((X_train, perturbated_x), axis=0)\n",
    "        new_y_train = np.concatenate((y_train, y_tmp_onehot_list), axis=0)\n",
    "        hist = model.fit(new_X_train, new_y_train, validation_split=0.01,\n",
    "                         batch_size=5000,epochs=10,verbose=0,callbacks=[es_unc])\n",
    "\n",
    "        y_hat = model.predict(X_test)\n",
    "\n",
    "\n",
    "    prec_list.append(precision_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "    recall_list.append(recall_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "    f1_list.append(f1_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "    acc_list.append(accuracy_score(y_test.argmax(axis=1),y_hat.argmax(axis=1)))\n",
    "print(\"beta: \",beta,\"   alpha:  \",alpha)\n",
    "sns.set()\n",
    "f, ax = plt.subplots(1,4, figsize=(12, 2))\n",
    "ax[0].plot(acc_list,label='Accuracy')\n",
    "ax[1].plot(prec_list,label='Precision')\n",
    "ax[2].plot(recall_list,label='Recall')\n",
    "ax[3].plot(f1_list,label='F1')\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[2].legend()\n",
    "ax[3].legend()\n",
    "plt.savefig(f'Accuracy_Precision_Recall_{alpha}_{beta}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JL_a9Hnrssix",
    "outputId": "89cc4e74-1bed-4794-e5cb-b9b74cbfad68",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "robust_predictions_train = model.predict_quantified(X_train,\n",
    "                                                   quantifier=quantifiers,\n",
    "                                                   batch_size=50000,\n",
    "                                                   sample_size=T,\n",
    "                                                   verbose=0)\n",
    "robust_predictions_test = model.predict_quantified(X_test,\n",
    "                                                   quantifier=quantifiers,\n",
    "                                                   batch_size=50000,\n",
    "                                                   sample_size=T,\n",
    "                                                   verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6moXECygWYH",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Adversarial Machine Learning based Test Input Generation\n",
    "\n",
    "adv_model = get_model()\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "\n",
    "hist = adv_model.fit(X_train, y_train, validation_split=0.3, batch_size=5000,\n",
    "                     epochs=200, verbose=0, callbacks=[es])\n",
    "\n",
    "init_predictions_train = adv_model.predict_quantified(X_train,\n",
    "                                                   quantifier=quantifiers,\n",
    "                                                   batch_size=50000,\n",
    "                                                   sample_size=T,\n",
    "                                                   verbose=0)\n",
    "init_predictions_test = adv_model.predict_quantified(X_test,\n",
    "                                                   quantifier=quantifiers,\n",
    "                                                   batch_size=50000,\n",
    "                                                   sample_size=T,\n",
    "                                                   verbose=0)\n",
    "\n",
    "\n",
    "with open('{alpha}_{beta}.pickle', 'wb') as handle:\n",
    "    pickle.dump(hist, handle)\n",
    "plot_history(hist)\n",
    "plt.savefig(f'hist_{alpha}_{beta}.png')\n",
    "adv_model.inner.load_weights('model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "axes = ['Init Train', 'After Train','Init Test','After Test']\n",
    "vals = [np.mean(init_predictions_train[0][1]),np.mean(robust_predictions_train[0][1]),\n",
    "        np.mean(init_predictions_test[0][1]),np.mean(robust_predictions_test[0][1]) ]\n",
    "plt.bar(axes,vals)\n",
    "plt.title('Average Uncertainty')\n",
    "plt.savefig(f'Average Uncertainty_{alpha}_{beta}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4M_meG7BghYp",
    "outputId": "a4d14ab6-da23-4661-9003-50aae23ac852",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "norm = np.inf\n",
    "\n",
    "#model.inner.load_weights('model.hdf5')\n",
    "\n",
    "es_adv = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "\n",
    "logits_model = tf.keras.Model(adv_model.inner.input, adv_model.inner.layers[-1].output)\n",
    "\n",
    "y_hat = adv_model.predict(X_test)\n",
    "\n",
    "adv_prec_list = []\n",
    "adv_recall_list = []\n",
    "adv_f1_list = []\n",
    "adv_acc_list = []\n",
    "\n",
    "adv_prec_list.append(precision_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "adv_recall_list.append(recall_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "adv_f1_list.append(f1_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "adv_acc_list.append(accuracy_score(y_test.argmax(axis=1),y_hat.argmax(axis=1)))\n",
    "\n",
    "for i in tqdm(range(EXPERIMENT_SIZE)):\n",
    "    perturbated_x = fast_gradient_method(logits_model, X_train[0:SAMPLE_SIZE,:], noise_budget,\n",
    "                                               norm, targeted=False)\n",
    "\n",
    "    new_X_train = np.concatenate((X_train, perturbated_x), axis=0)\n",
    "    new_y_train = np.concatenate((y_train, y_train[0:SAMPLE_SIZE]), axis=0)\n",
    "\n",
    "    model.inner.load_weights('model.hdf5')\n",
    "    \n",
    "    hist = adv_model.fit(new_X_train, new_y_train, validation_split=0.3,\n",
    "                         batch_size=5000,epochs=5,verbose=0,callbacks=[es_adv])\n",
    "    \n",
    "    y_hat = adv_model.predict(X_test)\n",
    "        \n",
    "    adv_prec_list.append(precision_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "    adv_recall_list.append(recall_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "    adv_f1_list.append(f1_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "    adv_acc_list.append(accuracy_score(y_test.argmax(axis=1),y_hat.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "huVazWB4gmtU",
    "outputId": "c9ba0fcd-6616-4d0a-ceb4-12e07dae9bd5",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "f, ax = plt.subplots(1,4, figsize=(12, 2))\n",
    "ax[0].plot(adv_acc_list,label='Accuracy')\n",
    "ax[1].plot(adv_prec_list,label='Precision')\n",
    "ax[2].plot(adv_recall_list,label='Recall')\n",
    "ax[3].plot(adv_f1_list,label='F1')\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[2].legend()\n",
    "ax[3].legend()\n",
    "plt.savefig(f'Accuracy_Precision_Recall_2{alpha}_{beta}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ID4WP0R7gn5D",
    "outputId": "cdbff17f-4cde-45d3-aa64-63a645234e28",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "adv_robust_predictions_train = adv_model.predict_quantified(X_train,\n",
    "                                                   quantifier=quantifiers,\n",
    "                                                   batch_size=50000,\n",
    "                                                   sample_size=T,\n",
    "                                                   verbose=0)\n",
    "adv_robust_predictions_test = adv_model.predict_quantified(X_test,\n",
    "                                                   quantifier=quantifiers,\n",
    "                                                   batch_size=50000,\n",
    "                                                   sample_size=T,\n",
    "                                                   verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "axes = ['Init Train', 'After Train','Init Test','After Test']\n",
    "vals = [np.mean(init_predictions_train[0][1]),np.mean(adv_robust_predictions_train[0][1]),\n",
    "        np.mean(init_predictions_test[0][1]),np.mean(adv_robust_predictions_test[0][1]) ]\n",
    "plt.bar(axes,vals)\n",
    "plt.title('Average Uncertainty')\n",
    "plt.savefig(f'Average Uncertainty_2{alpha}_{beta}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dill.dump_session(f'notebook_env{alpha}_{beta}.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Untitled2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}