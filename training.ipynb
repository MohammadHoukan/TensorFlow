{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-07T14:56:54.631595Z",
     "iopub.status.busy": "2022-07-07T14:56:54.630726Z",
     "iopub.status.idle": "2022-07-07T14:57:49.289500Z",
     "shell.execute_reply": "2022-07-07T14:57:49.288755Z",
     "shell.execute_reply.started": "2022-07-07T14:56:54.631519Z"
    },
    "id": "WQlSJOuHYIax",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib==3.3.4\n",
      "  Downloading matplotlib-3.3.4-cp38-cp38-manylinux1_x86_64.whl (11.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy==1.19.5\n",
      "  Downloading numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pandas==1.4.0\n",
      "  Downloading pandas-1.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting seaborn==0.11.2\n",
      "  Downloading seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.8/292.8 KB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cleverhans==4.0.0\n",
      "  Downloading cleverhans-4.0.0-py3-none-any.whl (92 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.3/92.3 KB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pymoo==0.4.0\n",
      "  Downloading pymoo-0.4.0.tar.gz (511 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 KB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting scikit-learn==1.0.2\n",
      "  Downloading scikit_learn-1.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm==4.62.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (4.62.3)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 9)) (0.3.4)\n",
      "Collecting uncertainty_wizard\n",
      "  Downloading uncertainty_wizard-0.3.0-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.0/49.0 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting plot_keras_history\n",
      "  Downloading plot_keras_history-1.1.36.tar.gz (9.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gdown\n",
      "  Downloading gdown-4.5.1.tar.gz (14 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.3.4->-r requirements.txt (line 1)) (9.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.3.4->-r requirements.txt (line 1)) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.3.4->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.3.4->-r requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.3.4->-r requirements.txt (line 1)) (3.0.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas==1.4.0->-r requirements.txt (line 3)) (2021.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.8/dist-packages (from seaborn==0.11.2->-r requirements.txt (line 4)) (1.4.1)\n",
      "Collecting mnist\n",
      "  Downloading mnist-0.2.2-py2.py3-none-any.whl (3.5 kB)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from cleverhans==4.0.0->-r requirements.txt (line 5)) (1.1.0)\n",
      "Collecting easydict\n",
      "  Downloading easydict-1.9.tar.gz (6.4 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting nose\n",
      "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 KB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-probability\n",
      "  Downloading tensorflow_probability-0.17.0-py2.py3-none-any.whl (6.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from cleverhans==4.0.0->-r requirements.txt (line 5)) (1.15.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from cleverhans==4.0.0->-r requirements.txt (line 5)) (0.12.0)\n",
      "Collecting pycodestyle\n",
      "  Downloading pycodestyle-2.8.0-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting autograd>=1.3\n",
      "  Downloading autograd-1.4-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cma==2.7\n",
      "  Downloading cma-2.7.0-py2.py3-none-any.whl (239 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.1/239.1 KB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==1.0.2->-r requirements.txt (line 7)) (3.1.0)\n",
      "Collecting sanitize_ml_labels>=1.0.33\n",
      "  Downloading sanitize_ml_labels-1.0.43.tar.gz (321 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.1/321.1 KB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown->-r requirements.txt (line 12)) (2.27.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown->-r requirements.txt (line 12)) (3.4.2)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 KB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.8/dist-packages (from autograd>=1.3->pymoo==0.4.0->-r requirements.txt (line 6)) (0.18.2)\n",
      "Collecting compress_json\n",
      "  Downloading compress_json-1.0.7.tar.gz (5.4 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 12)) (2.0.11)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 12)) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 12)) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 12)) (3.3)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability->cleverhans==4.0.0->-r requirements.txt (line 5)) (2.0.0)\n",
      "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability->cleverhans==4.0.0->-r requirements.txt (line 5)) (0.4.0)\n",
      "Collecting dm-tree\n",
      "  Downloading dm_tree-0.1.7-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (142 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.6/142.6 KB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability->cleverhans==4.0.0->-r requirements.txt (line 5)) (5.1.1)\n",
      "Building wheels for collected packages: pymoo, plot_keras_history, gdown, sanitize_ml_labels, easydict, compress_json\n",
      "  Building wheel for pymoo (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pymoo: filename=pymoo-0.4.0-cp38-cp38-linux_x86_64.whl size=1741424 sha256=e307dd50acd004a55a7e09f2ecc938c7f1b98a90f6fa1c872c15a2723e560b52\n",
      "  Stored in directory: /root/.cache/pip/wheels/7c/5d/d1/b4d13eecdba15202701cca03cd5a4a70fe9a6fc56676e5d2a6\n",
      "  Building wheel for plot_keras_history (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for plot_keras_history: filename=plot_keras_history-1.1.36-py3-none-any.whl size=9238 sha256=f73f3ee94e6b9d357ef16246824823ba229c6807749d55b960a672e60636b73f\n",
      "  Stored in directory: /root/.cache/pip/wheels/23/57/9c/9361218151ba1b3372b10fdcc5be07966041bc29b30476b686\n",
      "  Building wheel for gdown (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gdown: filename=gdown-4.5.1-py3-none-any.whl size=14951 sha256=c9490c83e6a0d18fbf405ce3dbbd88d6a81c3971362c5183436e44042685f055\n",
      "  Stored in directory: /root/.cache/pip/wheels/8d/df/71/846b2aa0fabaac2af23fbc5214eeaa55f0616e9d1a05187d72\n",
      "  Building wheel for sanitize_ml_labels (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sanitize_ml_labels: filename=sanitize_ml_labels-1.0.43-py3-none-any.whl size=319231 sha256=760f23fcd3ba18870e152689dc8a5b3f76614f8c239fdc475116698392e2e89e\n",
      "  Stored in directory: /root/.cache/pip/wheels/2c/f4/1e/e9284df82478d244e6a01dc246bf6d50f15384777d2a80eb51\n",
      "  Building wheel for easydict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for easydict: filename=easydict-1.9-py3-none-any.whl size=6361 sha256=4e81cc43b1ba9f6078ac69301b3e2c6ddc3853f9394fdf9496ffafdbbc4e0b9b\n",
      "  Stored in directory: /root/.cache/pip/wheels/d3/e0/e9/305e348717e399665119bd012510d51ff4f22d709ff60c3096\n",
      "  Building wheel for compress_json (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for compress_json: filename=compress_json-1.0.7-py3-none-any.whl size=5234 sha256=e0975d99b93be7ffdb636f96c5fa53556f285984b7f0d471fd44b8d890524eaa\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/a2/2c/c9def2f8bc67f3bba74bf51218b5135be4daab11e34f947749\n",
      "Successfully built pymoo plot_keras_history gdown sanitize_ml_labels easydict compress_json\n",
      "Installing collected packages: nose, easydict, dm-tree, compress_json, cma, uncertainty_wizard, soupsieve, sanitize_ml_labels, PySocks, pycodestyle, numpy, tensorflow-probability, pandas, mnist, matplotlib, beautifulsoup4, autograd, seaborn, scikit-learn, pymoo, plot_keras_history, gdown, cleverhans\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.4\n",
      "    Uninstalling numpy-1.19.4:\n",
      "      Successfully uninstalled numpy-1.19.4\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.2.5\n",
      "    Uninstalling pandas-1.2.5:\n",
      "      Successfully uninstalled pandas-1.2.5\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.5.0\n",
      "    Uninstalling matplotlib-3.5.0:\n",
      "      Successfully uninstalled matplotlib-3.5.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.0\n",
      "    Uninstalling scikit-learn-0.24.0:\n",
      "      Successfully uninstalled scikit-learn-0.24.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dask-cudf 21.12.0a0+293.g0930f712e6 requires pandas<1.4.0dev0,>=1.0, but you have pandas 1.4.0 which is incompatible.\n",
      "cudf 21.12.0a0+293.g0930f712e6 requires pandas<1.4.0dev0,>=1.0, but you have pandas 1.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PySocks-1.7.1 autograd-1.4 beautifulsoup4-4.11.1 cleverhans-4.0.0 cma-2.7.0 compress_json-1.0.7 dm-tree-0.1.7 easydict-1.9 gdown-4.5.1 matplotlib-3.3.4 mnist-0.2.2 nose-1.3.7 numpy-1.19.5 pandas-1.4.0 plot_keras_history-1.1.36 pycodestyle-2.8.0 pymoo-0.4.0 sanitize_ml_labels-1.0.43 scikit-learn-1.0.2 seaborn-0.11.2 soupsieve-2.3.2.post1 tensorflow-probability-0.17.0 uncertainty_wizard-0.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-07T14:58:23.852996Z",
     "iopub.status.busy": "2022-07-07T14:58:23.852707Z",
     "iopub.status.idle": "2022-07-07T14:58:29.332307Z",
     "shell.execute_reply": "2022-07-07T14:58:29.331681Z",
     "shell.execute_reply.started": "2022-07-07T14:58:23.852953Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import uncertainty_wizard as uwiz\n",
    "from cleverhans.tf2.attacks.fast_gradient_method import fast_gradient_method\n",
    "from pymoo.algorithms.nsga2 import NSGA2\n",
    "from pymoo.factory import get_sampling, get_crossover, get_mutation\n",
    "from pymoo.model.problem import Problem\n",
    "from pymoo.optimize import minimize\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from plot_keras_history import plot_history\n",
    "import warnings\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-07T14:59:03.490010Z",
     "iopub.status.busy": "2022-07-07T14:59:03.489759Z",
     "iopub.status.idle": "2022-07-07T14:59:03.494063Z",
     "shell.execute_reply": "2022-07-07T14:59:03.493533Z",
     "shell.execute_reply.started": "2022-07-07T14:59:03.489984Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('data.txt') as f:\n",
    "    data=f.readlines()\n",
    "data[1]=data[1].split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-07T15:14:27.562134Z",
     "iopub.status.busy": "2022-07-07T15:14:27.561577Z",
     "iopub.status.idle": "2022-07-07T15:14:43.840869Z",
     "shell.execute_reply": "2022-07-07T15:14:43.839947Z",
     "shell.execute_reply.started": "2022-07-07T15:14:27.562105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  p7zip\n",
      "Suggested packages:\n",
      "  p7zip-rar\n",
      "The following NEW packages will be installed:\n",
      "  p7zip p7zip-full\n",
      "0 upgraded, 2 newly installed, 0 to remove and 35 not upgraded.\n",
      "Need to get 1545 kB of archives.\n",
      "After this operation, 5896 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 p7zip amd64 16.02+dfsg-7build1 [358 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 p7zip-full amd64 16.02+dfsg-7build1 [1187 kB]\n",
      "Fetched 1545 kB in 1s (1903 kB/s)   \n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Selecting previously unselected package p7zip.\n",
      "(Reading database ... 41944 files and directories currently installed.)\n",
      "Preparing to unpack .../p7zip_16.02+dfsg-7build1_amd64.deb ...\n",
      "Unpacking p7zip (16.02+dfsg-7build1) ...\n",
      "Selecting previously unselected package p7zip-full.\n",
      "Preparing to unpack .../p7zip-full_16.02+dfsg-7build1_amd64.deb ...\n",
      "Unpacking p7zip-full (16.02+dfsg-7build1) ...\n",
      "Setting up p7zip (16.02+dfsg-7build1) ...\n",
      "Setting up p7zip-full (16.02+dfsg-7build1) ...\n",
      "\n",
      "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
      "p7zip Version 16.02 (locale=C.UTF-8,Utf16=on,HugeFiles=on,64 bits,8 CPUs Intel(R) Xeon(R) CPU E5-2623 v4 @ 2.60GHz (406F1),ASM,AES-NI)\n",
      "\n",
      "Scanning the drive for archives:\n",
      "  0M Sca        1 file, 10485760 bytes (10 MiB)\n",
      "\n",
      "Extracting archive: X.zip.001\n",
      "  0% 1 Ope          --\n",
      "Path = X.zip.001\n",
      "Type = Split\n",
      "Physical Size = 10485760\n",
      "Volumes = 54\n",
      "Total Physical Size = 560330856\n",
      "----\n",
      "Path = X.zip\n",
      "Size = 560330856\n",
      "--\n",
      "Path = X.zip\n",
      "Type = zip\n",
      "Physical Size = 560330856\n",
      "\n",
      "      0% -        1% -        3% -        5% -        7% -        9% -       11% -       13% -       15% -       18% -       20% -       22% -       24% -       26% -       29% -       31% -       33% -       36% -       39% -       42% -       44% -       46% -       48% -       50% -       51% -       53% -       55% -       57% -       58% -       60% -       62% -       63% -       65% -       67% -       69% -       70% -       72% -       73% -       75% -       76% -       78% -       79% -       81% -       83% -       84% -       86% -       87% -       89% -       90% -       92% -       93% -       95% -       97% -       99% -      Everything is Ok\n",
      "\n",
      "Size:       1964577621\n",
      "Compressed: 560330856\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y p7zip-full\n",
    "!7z x X.zip.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_SIZE=int(data[1][0])\n",
    "SAMPLE_SIZE =int(data[1][1])\n",
    "percentage=float(data[1][2])\n",
    "noise_budget =float(data[1][3])\n",
    "T=int(data[1][4])\n",
    "alpha=float(data[1][5])\n",
    "beta=float(data[1][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 40 1.0 0.3 40 0.9 0.3\n"
     ]
    }
   ],
   "source": [
    "print(EXPERIMENT_SIZE,SAMPLE_SIZE,percentage,noise_budget,T,alpha,beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T23:26:31.497770Z",
     "iopub.status.busy": "2022-07-04T23:26:31.497353Z",
     "iopub.status.idle": "2022-07-04T23:26:31.503952Z",
     "shell.execute_reply": "2022-07-04T23:26:31.502606Z",
     "shell.execute_reply.started": "2022-07-04T23:26:31.497770Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9 0.3 40 40\n"
     ]
    }
   ],
   "source": [
    "print(alpha,beta,EXPERIMENT_SIZE,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T23:26:32.378179Z",
     "iopub.status.busy": "2022-07-04T23:26:32.377832Z",
     "iopub.status.idle": "2022-07-04T23:26:32.391243Z",
     "shell.execute_reply": "2022-07-04T23:26:32.389744Z",
     "shell.execute_reply.started": "2022-07-04T23:26:32.378143Z"
    },
    "id": "NaQQiEC2QkGl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class UncertaintyMaximization(Problem):\n",
    "    def __init__(self,input_instance,y, epsilon,beta,alpha):\n",
    "        super().__init__(n_var=X_train.shape[1],\n",
    "                         n_obj=1,\n",
    "                         n_constr=0,\n",
    "                         elementwise_evaluation=True)\n",
    "        self.input_instance = input_instance\n",
    "        self.y = y\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha=alpha\n",
    "        self.beta=beta\n",
    "    def _evaluate(self, noise, out, *args, **kwargs):\n",
    "        prev_results = model.predict_quantified(self.input_instance,\n",
    "                                       quantifier=quantifiers,\n",
    "                                       batch_size=50000,\n",
    "                                       sample_size=T,\n",
    "                                       verbose=0)\n",
    "        \n",
    "        tmp_instance = self.input_instance + noise\n",
    "        results = model.predict_quantified(tmp_instance,\n",
    "                                       quantifier=quantifiers,\n",
    "                                       batch_size=50000,\n",
    "                                       sample_size=T,\n",
    "                                       verbose=0)\n",
    "        unc = results[0][1][0]\n",
    "        y_hat = results[0][0][0]\n",
    "        f1 = self.alpha/(0.0000001 + unc) + 0.01 * np.linalg.norm(noise) + self.beta * (np.int(self.y)==np.int(y_hat))\n",
    "        g1 = np.linalg.norm(noise, ord=np.inf) - self.epsilon\n",
    "        out[\"F\"] = np.column_stack([f1])\n",
    "        out[\"G\"] = np.column_stack([g1])\n",
    "        \n",
    "        #print('U1:', prev_results[0][1][0],'U2:', unc,'Obj:',f1)\n",
    "def get_bounds(uncertainty_budget):\n",
    "    distance = np.max(X_train) - np.min(X_train)\n",
    "    b = (-1*distance*uncertainty_budget/2.0, distance*uncertainty_budget/2.0)\n",
    "\n",
    "    bounds = [b for i in range(X_train.shape[1])]\n",
    "\n",
    "    initial_noise = np.random.uniform(low=np.min(bounds),\n",
    "                                      high=np.max(bounds),\n",
    "                                      size=(X_train.shape[1],))\n",
    "\n",
    "    return bounds,initial_noise\n",
    "def get_model():\n",
    "    model = uwiz.models.StochasticSequential()\n",
    "    model.add(tf.keras.layers.Dense(100, activation='relu',input_dim=X_train.shape[1]))\n",
    "    model.add(tf.keras.layers.Dropout(0.09))\n",
    "    model.add(tf.keras.layers.Dense(200, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.09))\n",
    "    model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'],run_eagerly=True)\n",
    "    return model\n",
    "def generate_instances(beta,alpha,noise_budget = 0.05, sample_size = 10):#beta=0.1,alpha=0.1):\n",
    "    bounds,initial_noise = get_bounds(noise_budget)\n",
    "    prev_unc_list = []\n",
    "    unc_list = []\n",
    "    prev_pred_list = []\n",
    "    pred_list = []\n",
    "    noise_norm = []\n",
    "    org_x = []\n",
    "    perturbated_x = []\n",
    "    original_y = []\n",
    "    input_norm = []\n",
    "    selected_idx = []\n",
    "    y_tmp_onehot_list = []\n",
    "\n",
    "    for j in tqdm(range(sample_size)):\n",
    "        #i = np.random.randint(0,X_train.shape[0],1)[0]\n",
    "        i=j\n",
    "        #print('i',i)\n",
    "        selected_idx.append(i)\n",
    "        x_tmp = np.array([X_train[i,:],])\n",
    "        y_tmp = y_train[i].argmax()\n",
    "        y_tmp_onehot = y_train[i]\n",
    "\n",
    "        prev_results = model.predict_quantified(x_tmp,\n",
    "                                               quantifier=quantifiers,\n",
    "                                               batch_size=50000,\n",
    "                                               sample_size=30,\n",
    "                                               verbose=0)\n",
    "        problem = UncertaintyMaximization(input_instance=x_tmp,\n",
    "                                          y=y_tmp,\n",
    "                                          epsilon=noise_budget,alpha=alpha,beta=beta)\n",
    "        problem.xl = np.ones((x_tmp.shape[1],)) * np.min(bounds)\n",
    "        problem.xu = np.ones((x_tmp.shape[1],)) * np.max(bounds)\n",
    "\n",
    "        algorithm = NSGA2(\n",
    "                            pop_size=40,\n",
    "                            n_offsprings=10,\n",
    "                            sampling=get_sampling(\"real_random\"),\n",
    "                            crossover=get_crossover(\"real_sbx\", prob=0.9, eta=15),\n",
    "                            mutation=get_mutation(\"real_pm\", eta=20),\n",
    "                            eliminate_duplicates=True\n",
    "                        )\n",
    "\n",
    "        res = minimize(problem,\n",
    "                       algorithm,\n",
    "                       (\"n_gen\", 10),\n",
    "                       verbose=False)\n",
    "\n",
    "        ##############################\n",
    "        ###     Noise generated    ###\n",
    "        ##############################\n",
    "\n",
    "        res1 = model.predict_quantified(x_tmp,\n",
    "                                               quantifier=quantifiers,\n",
    "                                               batch_size=50000,\n",
    "                                               sample_size=T,\n",
    "                                               verbose=0)\n",
    "        x_tmp_noisy = x_tmp + res.X\n",
    "\n",
    "        results = model.predict_quantified(x_tmp_noisy,\n",
    "                                           quantifier=quantifiers,\n",
    "                                           batch_size=50000,\n",
    "                                           sample_size=T,\n",
    "                                           verbose=0)\n",
    "        #print('U1:', res1[0][1][0],'U2:', results[0][1][0])\n",
    "        prev_unc_list.append(prev_results[0][1][0])\n",
    "        prev_pred_list.append(prev_results[0][0][0].astype(int))\n",
    "\n",
    "        unc_list.append(results[0][1][0])\n",
    "        pred_list.append(results[0][0][0].astype(int))\n",
    "\n",
    "        noise_norm.append(np.linalg.norm(res.X))\n",
    "        input_norm.append(np.linalg.norm(x_tmp))\n",
    "        original_y.append(y_tmp)\n",
    "        y_tmp_onehot_list.append(y_tmp_onehot)\n",
    "\n",
    "        org_x.append(x_tmp)\n",
    "        perturbated_x.append(x_tmp_noisy)\n",
    "\n",
    "    df = pd.DataFrame({'idx':selected_idx,'y':original_y,\n",
    "                       'y_hat':prev_pred_list,'y_hat2':pred_list,\n",
    "                       'unc_init':prev_unc_list,'unc_pert':unc_list,\n",
    "                       'input_norm':input_norm,'noise_norm':noise_norm\n",
    "                    })\n",
    "    np.savez(f'train_test_{alpha}_{beta}-' + str(noise_budget) + '.npz',\n",
    "                     X_train=X_train, X_test=X_test, y_train=y_train,\n",
    "                     y_test=y_test, org_x=org_x, perturbated_x=perturbated_x,\n",
    "                     original_y=original_y,prev_pred_list=prev_pred_list,\n",
    "                     pred_list=pred_list,y_tmp_onehot_list=y_tmp_onehot_list)\n",
    "    df.to_csv(f'manipulated_instances_{alpha}_{beta}.csv',index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T23:26:34.245731Z",
     "iopub.status.busy": "2022-07-04T23:26:34.245397Z",
     "iopub.status.idle": "2022-07-04T23:26:35.769046Z",
     "shell.execute_reply": "2022-07-04T23:26:35.768122Z",
     "shell.execute_reply.started": "2022-07-04T23:26:34.245703Z"
    },
    "id": "A53rJo6wTEr6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245818\n"
     ]
    }
   ],
   "source": [
    "# demonstrate data normalization with sklearn\n",
    "# load data\n",
    "data = ...\n",
    "# create scaler\n",
    "scaler = MinMaxScaler()\n",
    "infileX = open(\"X\",'rb')\n",
    "X = pickle.load(infileX)\n",
    "infiley = open(\"y\",'rb')\n",
    "y = pickle.load(infiley)\n",
    "length=int(len(X)*percentage)\n",
    "Xs=X[:length]\n",
    "Ys=y[:length]\n",
    "print(len(Ys))\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, Ys,\n",
    "                                                    test_size=0.33, random_state=27)\n",
    "quantifiers = ['pred_entropy']\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T23:26:43.719291Z",
     "iopub.status.busy": "2022-07-04T23:26:43.719001Z"
    },
    "id": "3a161rZyhfKE",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      9037\n",
      "           1       0.00      0.00      0.00      9098\n",
      "           2       0.29      0.00      0.01      8853\n",
      "           3       0.00      0.00      0.00      9062\n",
      "           4       0.00      0.00      0.00      9020\n",
      "           5       0.00      0.00      0.00      9141\n",
      "           6       0.10      0.84      0.18      8909\n",
      "           7       0.00      0.00      0.00      9052\n",
      "           8       0.17      0.12      0.14      8948\n",
      "\n",
      "    accuracy                           0.11     81120\n",
      "   macro avg       0.06      0.11      0.04     81120\n",
      "weighted avg       0.06      0.11      0.04     81120\n",
      "\n",
      "**************************************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc27b3713318451b9f87729d7e464654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698352d2d9c94e988a096a6fe68beb90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314ab7425fe34256bf70f0a665e41316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63d4ca6732749ab889a12d745103282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50a6c848ae941dd92c6b27a2c9aa53e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39effe0626b41718909b05ac62966cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f052b4e5150b444ab0092dabcf912450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00005: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8298838a1c244c8e8d367ba0d386af5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3eef66f2ddb46a39357ea5665606dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00008: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679bc2e2233c43a4a5fee87402c840ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d8d38ccaa34e2b84a78aa5463918c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Generate instances\n",
    "tf.config.run_functions_eagerly(True)\n",
    "y_hat = model.predict(X_test)\n",
    "cr = classification_report(y_test.argmax(axis=1),y_hat.argmax(axis=1))\n",
    "print(cr)\n",
    "print('*'*50)\n",
    "prec_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "acc_list = []\n",
    "\n",
    "prec_list.append(precision_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "recall_list.append(recall_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "f1_list.append(f1_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "acc_list.append(accuracy_score(y_test.argmax(axis=1),y_hat.argmax(axis=1)))\n",
    "\n",
    "\n",
    "es_unc = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "\n",
    "model.inner.load_weights('model.hdf5')\n",
    "\n",
    "for _ in range(EXPERIMENT_SIZE):\n",
    "    df = generate_instances(noise_budget = noise_budget, sample_size=T,beta=beta,alpha=alpha)\n",
    "    idx = df.idx.values\n",
    "    if len(idx) > 0:\n",
    "        load_data = np.load(f'train_test_{alpha}_{beta}-' + str(noise_budget) + '.npz')\n",
    "        perturbated_x = load_data['perturbated_x']\n",
    "        original_y = load_data['original_y']\n",
    "        X_train = load_data['X_train']\n",
    "        y_train = load_data['y_train']\n",
    "        X_test = load_data['X_test']\n",
    "        y_test = load_data['y_test']\n",
    "        y_tmp_onehot_list = load_data['y_tmp_onehot_list']\n",
    "\n",
    "        perturbated_x = perturbated_x.reshape((perturbated_x.shape[0],perturbated_x.shape[2]))\n",
    "\n",
    "        new_X_train = np.concatenate((X_train, perturbated_x), axis=0)\n",
    "        new_y_train = np.concatenate((y_train, y_tmp_onehot_list), axis=0)\n",
    "        hist = model.fit(new_X_train, new_y_train, validation_split=0.01,\n",
    "                         batch_size=5000,epochs=10,verbose=0,callbacks=[es_unc])\n",
    "\n",
    "        y_hat = model.predict(X_test)\n",
    "\n",
    "\n",
    "    prec_list.append(precision_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "    recall_list.append(recall_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "    f1_list.append(f1_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "    acc_list.append(accuracy_score(y_test.argmax(axis=1),y_hat.argmax(axis=1)))\n",
    "print(\"beta: \",beta,\"   alpha:  \",alpha)\n",
    "sns.set()\n",
    "f, ax = plt.subplots(1,4, figsize=(12, 2))\n",
    "ax[0].plot(acc_list,label='Accuracy')\n",
    "ax[1].plot(prec_list,label='Precision')\n",
    "ax[2].plot(recall_list,label='Recall')\n",
    "ax[3].plot(f1_list,label='F1')\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[2].legend()\n",
    "ax[3].legend()\n",
    "plt.savefig(f'Accuracy_Precision_Recall_{alpha}_{beta}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JL_a9Hnrssix",
    "outputId": "89cc4e74-1bed-4794-e5cb-b9b74cbfad68",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "robust_predictions_train = model.predict_quantified(X_train,\n",
    "                                                   quantifier=quantifiers,\n",
    "                                                   batch_size=50000,\n",
    "                                                   sample_size=T,\n",
    "                                                   verbose=0)\n",
    "robust_predictions_test = model.predict_quantified(X_test,\n",
    "                                                   quantifier=quantifiers,\n",
    "                                                   batch_size=50000,\n",
    "                                                   sample_size=T,\n",
    "                                                   verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6moXECygWYH",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Adversarial Machine Learning based Test Input Generation\n",
    "\n",
    "adv_model = get_model()\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "\n",
    "hist = adv_model.fit(X_train, y_train, validation_split=0.3, batch_size=5000,\n",
    "                     epochs=200, verbose=0, callbacks=[es])\n",
    "\n",
    "init_predictions_train = adv_model.predict_quantified(X_train,\n",
    "                                                   quantifier=quantifiers,\n",
    "                                                   batch_size=50000,\n",
    "                                                   sample_size=T,\n",
    "                                                   verbose=0)\n",
    "init_predictions_test = adv_model.predict_quantified(X_test,\n",
    "                                                   quantifier=quantifiers,\n",
    "                                                   batch_size=50000,\n",
    "                                                   sample_size=T,\n",
    "                                                   verbose=0)\n",
    "\n",
    "\n",
    "with open('{alpha}_{beta}.pickle', 'wb') as handle:\n",
    "    pickle.dump(hist, handle)\n",
    "plot_history(hist)\n",
    "plt.savefig(f'hist_{alpha}_{beta}.png')\n",
    "adv_model.inner.load_weights('model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "axes = ['Init Train', 'After Train','Init Test','After Test']\n",
    "vals = [np.mean(init_predictions_train[0][1]),np.mean(robust_predictions_train[0][1]),\n",
    "        np.mean(init_predictions_test[0][1]),np.mean(robust_predictions_test[0][1]) ]\n",
    "plt.bar(axes,vals)\n",
    "plt.title('Average Uncertainty')\n",
    "plt.savefig(f'Average Uncertainty_{alpha}_{beta}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4M_meG7BghYp",
    "outputId": "a4d14ab6-da23-4661-9003-50aae23ac852",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "norm = np.inf\n",
    "\n",
    "#model.inner.load_weights('model.hdf5')\n",
    "\n",
    "es_adv = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "\n",
    "logits_model = tf.keras.Model(adv_model.inner.input, adv_model.inner.layers[-1].output)\n",
    "\n",
    "y_hat = adv_model.predict(X_test)\n",
    "\n",
    "adv_prec_list = []\n",
    "adv_recall_list = []\n",
    "adv_f1_list = []\n",
    "adv_acc_list = []\n",
    "\n",
    "adv_prec_list.append(precision_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "adv_recall_list.append(recall_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "adv_f1_list.append(f1_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "adv_acc_list.append(accuracy_score(y_test.argmax(axis=1),y_hat.argmax(axis=1)))\n",
    "\n",
    "for i in tqdm(range(EXPERIMENT_SIZE)):\n",
    "    perturbated_x = fast_gradient_method(logits_model, X_train[0:SAMPLE_SIZE,:], noise_budget,\n",
    "                                               norm, targeted=False)\n",
    "\n",
    "    new_X_train = np.concatenate((X_train, perturbated_x), axis=0)\n",
    "    new_y_train = np.concatenate((y_train, y_train[0:SAMPLE_SIZE]), axis=0)\n",
    "\n",
    "    model.inner.load_weights('model.hdf5')\n",
    "    \n",
    "    hist = adv_model.fit(new_X_train, new_y_train, validation_split=0.3,\n",
    "                         batch_size=5000,epochs=5,verbose=0,callbacks=[es_adv])\n",
    "    \n",
    "    y_hat = adv_model.predict(X_test)\n",
    "        \n",
    "    adv_prec_list.append(precision_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "    adv_recall_list.append(recall_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "    adv_f1_list.append(f1_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "    adv_acc_list.append(accuracy_score(y_test.argmax(axis=1),y_hat.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "huVazWB4gmtU",
    "outputId": "c9ba0fcd-6616-4d0a-ceb4-12e07dae9bd5",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "f, ax = plt.subplots(1,4, figsize=(12, 2))\n",
    "ax[0].plot(adv_acc_list,label='Accuracy')\n",
    "ax[1].plot(adv_prec_list,label='Precision')\n",
    "ax[2].plot(adv_recall_list,label='Recall')\n",
    "ax[3].plot(adv_f1_list,label='F1')\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[2].legend()\n",
    "ax[3].legend()\n",
    "plt.savefig(f'Accuracy_Precision_Recall_2{alpha}_{beta}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ID4WP0R7gn5D",
    "outputId": "cdbff17f-4cde-45d3-aa64-63a645234e28",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "adv_robust_predictions_train = adv_model.predict_quantified(X_train,\n",
    "                                                   quantifier=quantifiers,\n",
    "                                                   batch_size=50000,\n",
    "                                                   sample_size=T,\n",
    "                                                   verbose=0)\n",
    "adv_robust_predictions_test = adv_model.predict_quantified(X_test,\n",
    "                                                   quantifier=quantifiers,\n",
    "                                                   batch_size=50000,\n",
    "                                                   sample_size=T,\n",
    "                                                   verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "axes = ['Init Train', 'After Train','Init Test','After Test']\n",
    "vals = [np.mean(init_predictions_train[0][1]),np.mean(adv_robust_predictions_train[0][1]),\n",
    "        np.mean(init_predictions_test[0][1]),np.mean(adv_robust_predictions_test[0][1]) ]\n",
    "plt.bar(axes,vals)\n",
    "plt.title('Average Uncertainty')\n",
    "plt.savefig(f'Average Uncertainty_2{alpha}_{beta}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dill.dump_session(f'notebook_env{alpha}_{beta}.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Untitled2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
