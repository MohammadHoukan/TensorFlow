{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T22:53:37.810890Z",
     "iopub.status.busy": "2022-07-04T22:53:37.810583Z",
     "iopub.status.idle": "2022-07-04T22:53:49.830615Z",
     "shell.execute_reply": "2022-07-04T22:53:49.829393Z",
     "shell.execute_reply.started": "2022-07-04T22:53:37.810867Z"
    },
    "id": "WQlSJOuHYIax",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import uncertainty_wizard as uwiz\n",
    "from cleverhans.tf2.attacks.fast_gradient_method import fast_gradient_method\n",
    "from pymoo.algorithms.nsga2 import NSGA2\n",
    "from pymoo.factory import get_sampling, get_crossover, get_mutation\n",
    "from pymoo.model.problem import Problem\n",
    "from pymoo.optimize import minimize\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from plot_keras_history import plot_history\n",
    "import warnings\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T23:26:30.217315Z",
     "iopub.status.busy": "2022-07-04T23:26:30.217030Z",
     "iopub.status.idle": "2022-07-04T23:26:30.221976Z",
     "shell.execute_reply": "2022-07-04T23:26:30.221087Z",
     "shell.execute_reply.started": "2022-07-04T23:26:30.217315Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('data.txt') as f:\n",
    "    data=f.readlines()\n",
    "data[1]=data[1].split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "EXPERIMENT_SIZE=int(data[1][0])\n",
    "SAMPLE_SIZE =int(data[1][1])\n",
    "percentage=float(data[1][2])\n",
    "noise_budget =float(data[1][3])\n",
    "T=int(data[1][4])\n",
    "alpha=float(data[1][5])\n",
    "beta=float(data[1][6])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T23:26:31.497770Z",
     "iopub.status.busy": "2022-07-04T23:26:31.497353Z",
     "iopub.status.idle": "2022-07-04T23:26:31.503952Z",
     "shell.execute_reply": "2022-07-04T23:26:31.502606Z",
     "shell.execute_reply.started": "2022-07-04T23:26:31.497770Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T23:26:32.378179Z",
     "iopub.status.busy": "2022-07-04T23:26:32.377832Z",
     "iopub.status.idle": "2022-07-04T23:26:32.391243Z",
     "shell.execute_reply": "2022-07-04T23:26:32.389744Z",
     "shell.execute_reply.started": "2022-07-04T23:26:32.378143Z"
    },
    "id": "NaQQiEC2QkGl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class UncertaintyMaximization(Problem):\n",
    "    def __init__(self,input_instance,y, epsilon,beta,alpha):\n",
    "        super().__init__(n_var=X_train.shape[1],\n",
    "                         n_obj=1,\n",
    "                         n_constr=0,\n",
    "                         elementwise_evaluation=True)\n",
    "        self.input_instance = input_instance\n",
    "        self.y = y\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha=alpha\n",
    "        self.beta=beta\n",
    "    def _evaluate(self, noise, out, *args, **kwargs):\n",
    "        prev_results = model.predict_quantified(self.input_instance,\n",
    "                                       quantifier=quantifiers,\n",
    "                                       batch_size=50000,\n",
    "                                       sample_size=T,\n",
    "                                       verbose=0)\n",
    "        \n",
    "        tmp_instance = self.input_instance + noise\n",
    "        results = model.predict_quantified(tmp_instance,\n",
    "                                       quantifier=quantifiers,\n",
    "                                       batch_size=50000,\n",
    "                                       sample_size=T,\n",
    "                                       verbose=0)\n",
    "        unc = results[0][1][0]\n",
    "        y_hat = results[0][0][0]\n",
    "        f1 = self.alpha/(0.0000001 + unc) + 0.01 * np.linalg.norm(noise) + self.beta * (np.int(self.y)==np.int(y_hat))\n",
    "        g1 = np.linalg.norm(noise, ord=np.inf) - self.epsilon\n",
    "        out[\"F\"] = np.column_stack([f1])\n",
    "        out[\"G\"] = np.column_stack([g1])\n",
    "        \n",
    "        #print('U1:', prev_results[0][1][0],'U2:', unc,'Obj:',f1)\n",
    "def get_bounds(uncertainty_budget):\n",
    "    distance = np.max(X_train) - np.min(X_train)\n",
    "    b = (-1*distance*uncertainty_budget/2.0, distance*uncertainty_budget/2.0)\n",
    "\n",
    "    bounds = [b for i in range(X_train.shape[1])]\n",
    "\n",
    "    initial_noise = np.random.uniform(low=np.min(bounds),\n",
    "                                      high=np.max(bounds),\n",
    "                                      size=(X_train.shape[1],))\n",
    "\n",
    "    return bounds,initial_noise\n",
    "def get_model():\n",
    "    model = uwiz.models.StochasticSequential()\n",
    "    model.add(tf.keras.layers.Dense(100, activation='relu',input_dim=X_train.shape[1]))\n",
    "    model.add(tf.keras.layers.Dropout(0.09))\n",
    "    model.add(tf.keras.layers.Dense(200, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.09))\n",
    "    model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'],run_eagerly=True)\n",
    "    return model\n",
    "def generate_instances(beta,alpha,noise_budget = 0.05, sample_size = 10):#beta=0.1,alpha=0.1):\n",
    "    bounds,initial_noise = get_bounds(noise_budget)\n",
    "    prev_unc_list = []\n",
    "    unc_list = []\n",
    "    prev_pred_list = []\n",
    "    pred_list = []\n",
    "    noise_norm = []\n",
    "    org_x = []\n",
    "    perturbated_x = []\n",
    "    original_y = []\n",
    "    input_norm = []\n",
    "    selected_idx = []\n",
    "    y_tmp_onehot_list = []\n",
    "\n",
    "    for j in tqdm(range(sample_size)):\n",
    "        #i = np.random.randint(0,X_train.shape[0],1)[0]\n",
    "        i=j\n",
    "        #print('i',i)\n",
    "        selected_idx.append(i)\n",
    "        x_tmp = np.array([X_train[i,:],])\n",
    "        y_tmp = y_train[i].argmax()\n",
    "        y_tmp_onehot = y_train[i]\n",
    "\n",
    "        prev_results = model.predict_quantified(x_tmp,\n",
    "                                               quantifier=quantifiers,\n",
    "                                               batch_size=50000,\n",
    "                                               sample_size=30,\n",
    "                                               verbose=0)\n",
    "        problem = UncertaintyMaximization(input_instance=x_tmp,\n",
    "                                          y=y_tmp,\n",
    "                                          epsilon=noise_budget,alpha=alpha,beta=beta)\n",
    "        problem.xl = np.ones((x_tmp.shape[1],)) * np.min(bounds)\n",
    "        problem.xu = np.ones((x_tmp.shape[1],)) * np.max(bounds)\n",
    "\n",
    "        algorithm = NSGA2(\n",
    "                            pop_size=40,\n",
    "                            n_offsprings=10,\n",
    "                            sampling=get_sampling(\"real_random\"),\n",
    "                            crossover=get_crossover(\"real_sbx\", prob=0.9, eta=15),\n",
    "                            mutation=get_mutation(\"real_pm\", eta=20),\n",
    "                            eliminate_duplicates=True\n",
    "                        )\n",
    "\n",
    "        res = minimize(problem,\n",
    "                       algorithm,\n",
    "                       (\"n_gen\", 10),\n",
    "                       verbose=False)\n",
    "\n",
    "        ##############################\n",
    "        ###     Noise generated    ###\n",
    "        ##############################\n",
    "\n",
    "        res1 = model.predict_quantified(x_tmp,\n",
    "                                               quantifier=quantifiers,\n",
    "                                               batch_size=50000,\n",
    "                                               sample_size=T,\n",
    "                                               verbose=0)\n",
    "        x_tmp_noisy = x_tmp + res.X\n",
    "\n",
    "        results = model.predict_quantified(x_tmp_noisy,\n",
    "                                           quantifier=quantifiers,\n",
    "                                           batch_size=50000,\n",
    "                                           sample_size=T,\n",
    "                                           verbose=0)\n",
    "        #print('U1:', res1[0][1][0],'U2:', results[0][1][0])\n",
    "        prev_unc_list.append(prev_results[0][1][0])\n",
    "        prev_pred_list.append(prev_results[0][0][0].astype(int))\n",
    "\n",
    "        unc_list.append(results[0][1][0])\n",
    "        pred_list.append(results[0][0][0].astype(int))\n",
    "\n",
    "        noise_norm.append(np.linalg.norm(res.X))\n",
    "        input_norm.append(np.linalg.norm(x_tmp))\n",
    "        original_y.append(y_tmp)\n",
    "        y_tmp_onehot_list.append(y_tmp_onehot)\n",
    "\n",
    "        org_x.append(x_tmp)\n",
    "        perturbated_x.append(x_tmp_noisy)\n",
    "\n",
    "    df = pd.DataFrame({'idx':selected_idx,'y':original_y,\n",
    "                       'y_hat':prev_pred_list,'y_hat2':pred_list,\n",
    "                       'unc_init':prev_unc_list,'unc_pert':unc_list,\n",
    "                       'input_norm':input_norm,'noise_norm':noise_norm\n",
    "                    })\n",
    "    np.savez(f'train_test_{alpha}_{beta}-' + str(noise_budget) + '.npz',\n",
    "                     X_train=X_train, X_test=X_test, y_train=y_train,\n",
    "                     y_test=y_test, org_x=org_x, perturbated_x=perturbated_x,\n",
    "                     original_y=original_y,prev_pred_list=prev_pred_list,\n",
    "                     pred_list=pred_list,y_tmp_onehot_list=y_tmp_onehot_list)\n",
    "    df.to_csv(f'manipulated_instances_{alpha}_{beta}.csv',index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T23:26:34.245731Z",
     "iopub.status.busy": "2022-07-04T23:26:34.245397Z",
     "iopub.status.idle": "2022-07-04T23:26:35.769046Z",
     "shell.execute_reply": "2022-07-04T23:26:35.768122Z",
     "shell.execute_reply.started": "2022-07-04T23:26:34.245703Z"
    },
    "id": "A53rJo6wTEr6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245818\n"
     ]
    }
   ],
   "source": [
    "# demonstrate data normalization with sklearn\n",
    "# load data\n",
    "data = ...\n",
    "# create scaler\n",
    "scaler = MinMaxScaler()\n",
    "infileX = open(\"datasets/xy/X\",'rb')\n",
    "X = pickle.load(infileX)\n",
    "infiley = open(\"datasets/xy/y\",'rb')\n",
    "y = pickle.load(infiley)\n",
    "length=int(len(X)*percentage)\n",
    "Xs=X[:length]\n",
    "Ys=y[:length]\n",
    "print(len(Ys))\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, Ys,\n",
    "                                                    test_size=0.33, random_state=27)\n",
    "quantifiers = ['pred_entropy']\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T23:26:43.719291Z",
     "iopub.status.busy": "2022-07-04T23:26:43.719001Z"
    },
    "id": "3a161rZyhfKE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      9037\n",
      "           1       0.00      0.00      0.00      9098\n",
      "           2       0.00      0.00      0.00      8853\n",
      "           3       0.11      0.99      0.20      9062\n",
      "           4       0.00      0.00      0.00      9020\n",
      "           5       0.00      0.00      0.00      9141\n",
      "           6       0.00      0.00      0.00      8909\n",
      "           7       0.00      0.00      0.00      9052\n",
      "           8       0.00      0.00      0.00      8948\n",
      "\n",
      "    accuracy                           0.11     81120\n",
      "   macro avg       0.01      0.11      0.02     81120\n",
      "weighted avg       0.01      0.11      0.02     81120\n",
      "\n",
      "**************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a2e237f60d7246109a395f01ada09d31"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiled modules for significant speedup can not be used!\n",
      "https://pymoo.org/installation.html#installation\n",
      "\n",
      "To disable this warning:\n",
      "from pymoo.configuration import Configuration\n",
      "Configuration.show_compile_hint = False\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.compat.v2.__internal__.distribute' has no attribute 'strategy_supports_no_merge_call'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [8]\u001B[0m, in \u001B[0;36m<cell line: 22>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     37\u001B[0m     new_X_train \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate((X_train, perturbated_x), axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     38\u001B[0m     new_y_train \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate((y_train, y_tmp_onehot_list), axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m---> 39\u001B[0m     hist \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_X_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnew_y_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     40\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mes_unc\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     42\u001B[0m     y_hat \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(X_test)\n\u001B[0;32m     45\u001B[0m prec_list\u001B[38;5;241m.\u001B[39mappend(precision_score(y_test\u001B[38;5;241m.\u001B[39margmax(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m),y_hat\u001B[38;5;241m.\u001B[39margmax(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m),average\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mweighted\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[0;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\keras\\optimizer_v2\\utils.py:33\u001B[0m, in \u001B[0;36mall_reduce_sum_gradients\u001B[1;34m(grads_and_vars)\u001B[0m\n\u001B[0;32m     31\u001B[0m filtered_grads_and_vars \u001B[38;5;241m=\u001B[39m filter_empty_gradients(grads_and_vars)\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m filtered_grads_and_vars:\n\u001B[1;32m---> 33\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__internal__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistribute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstrategy_supports_no_merge_call\u001B[49m():\n\u001B[0;32m     34\u001B[0m     grads \u001B[38;5;241m=\u001B[39m [pair[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m pair \u001B[38;5;129;01min\u001B[39;00m filtered_grads_and_vars]\n\u001B[0;32m     35\u001B[0m     reduced \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mdistribute\u001B[38;5;241m.\u001B[39mget_replica_context()\u001B[38;5;241m.\u001B[39mall_reduce(\n\u001B[0;32m     36\u001B[0m         tf\u001B[38;5;241m.\u001B[39mdistribute\u001B[38;5;241m.\u001B[39mReduceOp\u001B[38;5;241m.\u001B[39mSUM, grads)\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'tensorflow.compat.v2.__internal__.distribute' has no attribute 'strategy_supports_no_merge_call'"
     ]
    }
   ],
   "source": [
    "### Generate instances\n",
    "tf.config.run_functions_eagerly(True)\n",
    "y_hat = model.predict(X_test)\n",
    "cr = classification_report(y_test.argmax(axis=1),y_hat.argmax(axis=1))\n",
    "print(cr)\n",
    "print('*'*50)\n",
    "prec_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "acc_list = []\n",
    "\n",
    "prec_list.append(precision_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "recall_list.append(recall_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "f1_list.append(f1_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "acc_list.append(accuracy_score(y_test.argmax(axis=1),y_hat.argmax(axis=1)))\n",
    "\n",
    "\n",
    "es_unc = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "\n",
    "model.inner.load_weights('model.hdf5')\n",
    "\n",
    "for _ in range(EXPERIMENT_SIZE):\n",
    "    df = generate_instances(noise_budget = noise_budget, sample_size=T,beta=beta,alpha=alpha)\n",
    "    idx = df.idx.values\n",
    "    if len(idx) > 0:\n",
    "        load_data = np.load(f'train_test_{alpha}_{beta}-' + str(noise_budget) + '.npz')\n",
    "        perturbated_x = load_data['perturbated_x']\n",
    "        original_y = load_data['original_y']\n",
    "        X_train = load_data['X_train']\n",
    "        y_train = load_data['y_train']\n",
    "        X_test = load_data['X_test']\n",
    "        y_test = load_data['y_test']\n",
    "        y_tmp_onehot_list = load_data['y_tmp_onehot_list']\n",
    "\n",
    "        perturbated_x = perturbated_x.reshape((perturbated_x.shape[0],perturbated_x.shape[2]))\n",
    "\n",
    "        new_X_train = np.concatenate((X_train, perturbated_x), axis=0)\n",
    "        new_y_train = np.concatenate((y_train, y_tmp_onehot_list), axis=0)\n",
    "        hist = model.fit(new_X_train, new_y_train, validation_split=0.01,\n",
    "                         batch_size=5000,epochs=10,verbose=0,callbacks=[es_unc])\n",
    "\n",
    "        y_hat = model.predict(X_test)\n",
    "\n",
    "\n",
    "    prec_list.append(precision_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "    recall_list.append(recall_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "    f1_list.append(f1_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "    acc_list.append(accuracy_score(y_test.argmax(axis=1),y_hat.argmax(axis=1)))\n",
    "print(\"beta: \",beta,\"   alpha:  \",alpha)\n",
    "sns.set()\n",
    "f, ax = plt.subplots(1,4, figsize=(12, 2))\n",
    "ax[0].plot(acc_list,label='Accuracy')\n",
    "ax[1].plot(prec_list,label='Precision')\n",
    "ax[2].plot(recall_list,label='Recall')\n",
    "ax[3].plot(f1_list,label='F1')\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[2].legend()\n",
    "ax[3].legend()\n",
    "plt.savefig(f'Accuracy_Precision_Recall_{alpha}_{beta}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JL_a9Hnrssix",
    "outputId": "89cc4e74-1bed-4794-e5cb-b9b74cbfad68",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "robust_predictions_train = model.predict_quantified(X_train,\n",
    "                                                   quantifier=quantifiers,\n",
    "                                                   batch_size=50000,\n",
    "                                                   sample_size=T,\n",
    "                                                   verbose=0)\n",
    "robust_predictions_test = model.predict_quantified(X_test,\n",
    "                                                   quantifier=quantifiers,\n",
    "                                                   batch_size=50000,\n",
    "                                                   sample_size=T,\n",
    "                                                   verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6moXECygWYH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Adversarial Machine Learning based Test Input Generation\n",
    "\n",
    "adv_model = get_model()\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "\n",
    "hist = adv_model.fit(X_train, y_train, validation_split=0.3, batch_size=5000,\n",
    "                     epochs=200, verbose=0, callbacks=[es])\n",
    "\n",
    "init_predictions_train = adv_model.predict_quantified(X_train,\n",
    "                                                   quantifier=quantifiers,\n",
    "                                                   batch_size=50000,\n",
    "                                                   sample_size=T,\n",
    "                                                   verbose=0)\n",
    "init_predictions_test = adv_model.predict_quantified(X_test,\n",
    "                                                   quantifier=quantifiers,\n",
    "                                                   batch_size=50000,\n",
    "                                                   sample_size=T,\n",
    "                                                   verbose=0)\n",
    "\n",
    "\n",
    "with open('{alpha}_{beta}.pickle', 'wb') as handle:\n",
    "    pickle.dump(hist, handle)\n",
    "plot_history(hist)\n",
    "plt.savefig(f'hist_{alpha}_{beta}.png')\n",
    "adv_model.inner.load_weights('model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "axes = ['Init Train', 'After Train','Init Test','After Test']\n",
    "vals = [np.mean(init_predictions_train[0][1]),np.mean(robust_predictions_train[0][1]),\n",
    "        np.mean(init_predictions_test[0][1]),np.mean(robust_predictions_test[0][1]) ]\n",
    "plt.bar(axes,vals)\n",
    "plt.title('Average Uncertainty')\n",
    "plt.savefig(f'Average Uncertainty_{alpha}_{beta}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4M_meG7BghYp",
    "outputId": "a4d14ab6-da23-4661-9003-50aae23ac852",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "norm = np.inf\n",
    "\n",
    "#model.inner.load_weights('model.hdf5')\n",
    "\n",
    "es_adv = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "\n",
    "logits_model = tf.keras.Model(adv_model.inner.input, adv_model.inner.layers[-1].output)\n",
    "\n",
    "y_hat = adv_model.predict(X_test)\n",
    "\n",
    "adv_prec_list = []\n",
    "adv_recall_list = []\n",
    "adv_f1_list = []\n",
    "adv_acc_list = []\n",
    "\n",
    "adv_prec_list.append(precision_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "adv_recall_list.append(recall_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "adv_f1_list.append(f1_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "adv_acc_list.append(accuracy_score(y_test.argmax(axis=1),y_hat.argmax(axis=1)))\n",
    "\n",
    "for i in tqdm(range(EXPERIMENT_SIZE)):\n",
    "    perturbated_x = fast_gradient_method(logits_model, X_train[0:SAMPLE_SIZE,:], noise_budget,\n",
    "                                               norm, targeted=False)\n",
    "\n",
    "    new_X_train = np.concatenate((X_train, perturbated_x), axis=0)\n",
    "    new_y_train = np.concatenate((y_train, y_train[0:SAMPLE_SIZE]), axis=0)\n",
    "\n",
    "    model.inner.load_weights('model.hdf5')\n",
    "    \n",
    "    hist = adv_model.fit(new_X_train, new_y_train, validation_split=0.3,\n",
    "                         batch_size=5000,epochs=5,verbose=0,callbacks=[es_adv])\n",
    "    \n",
    "    y_hat = adv_model.predict(X_test)\n",
    "        \n",
    "    adv_prec_list.append(precision_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "    adv_recall_list.append(recall_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "    adv_f1_list.append(f1_score(y_test.argmax(axis=1),y_hat.argmax(axis=1),average='weighted'))\n",
    "    adv_acc_list.append(accuracy_score(y_test.argmax(axis=1),y_hat.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "huVazWB4gmtU",
    "outputId": "c9ba0fcd-6616-4d0a-ceb4-12e07dae9bd5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "f, ax = plt.subplots(1,4, figsize=(12, 2))\n",
    "ax[0].plot(adv_acc_list,label='Accuracy')\n",
    "ax[1].plot(adv_prec_list,label='Precision')\n",
    "ax[2].plot(adv_recall_list,label='Recall')\n",
    "ax[3].plot(adv_f1_list,label='F1')\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[2].legend()\n",
    "ax[3].legend()\n",
    "plt.savefig(f'Accuracy_Precision_Recall_2{alpha}_{beta}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ID4WP0R7gn5D",
    "outputId": "cdbff17f-4cde-45d3-aa64-63a645234e28",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "adv_robust_predictions_train = adv_model.predict_quantified(X_train,\n",
    "                                                   quantifier=quantifiers,\n",
    "                                                   batch_size=50000,\n",
    "                                                   sample_size=T,\n",
    "                                                   verbose=0)\n",
    "adv_robust_predictions_test = adv_model.predict_quantified(X_test,\n",
    "                                                   quantifier=quantifiers,\n",
    "                                                   batch_size=50000,\n",
    "                                                   sample_size=T,\n",
    "                                                   verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "axes = ['Init Train', 'After Train','Init Test','After Test']\n",
    "vals = [np.mean(init_predictions_train[0][1]),np.mean(adv_robust_predictions_train[0][1]),\n",
    "        np.mean(init_predictions_test[0][1]),np.mean(adv_robust_predictions_test[0][1]) ]\n",
    "plt.bar(axes,vals)\n",
    "plt.title('Average Uncertainty')\n",
    "plt.savefig(f'Average Uncertainty_2{alpha}_{beta}.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Untitled2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}